{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0v4-4cHwN5z9"
      },
      "source": [
        "Given code: \n",
        "1. import packages\n",
        "2. load image pairs img1 & img2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UxIiSV-yN5jv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load image pairs - you can change file names\n",
        "image1 = cv2.imread(\"ings\\IMG_7577.png\")\n",
        "image2 = cv2.imread(\"ings\\IMG_7578.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GS-5N7zbN5Rs"
      },
      "source": [
        "Problem 1: [code by yourself]\n",
        "\n",
        "Define a function `get_transform_from_keypoints()` that computes the 3x3 homogeneous transform H from keypoint matches with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (output/return): computed linear transform matrix\n",
        "* kpts_src (output/return): computed keypoints for img_src\n",
        "* dscrpt_src (output/return): computed descriptors for img_src\n",
        "* kpts_dst (output/return): computed keypoints for img_dst\n",
        "* dscrpt_dst (output/return): computed descriptors for img_dst\n",
        "* matches (output/return): keypoint matches determined from SIFT\n",
        "\n",
        "When computing the linear transform, follow this proceses:\n",
        "* detect SIFT keypoints and compute SIFT descriptors\n",
        "* find the linear transform matrix H using the matched keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9IheSHuUN5CR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[533.8938  589.7643 ]]\n",
            "\n",
            " [[603.7789  585.0748 ]]\n",
            "\n",
            " [[703.61804 712.7682 ]]\n",
            "\n",
            " [[738.87823 498.10538]]\n",
            "\n",
            " [[739.9495  589.2371 ]]\n",
            "\n",
            " [[793.863   547.1412 ]]]\n",
            "[[[300.97025 497.69577]]\n",
            "\n",
            " [[369.88748 510.37115]]\n",
            "\n",
            " [[429.61197 652.71844]]\n",
            "\n",
            " [[524.2998  460.36768]]\n",
            "\n",
            " [[499.8654  547.071  ]]\n",
            "\n",
            " [[563.82556 520.0668 ]]]\n"
          ]
        }
      ],
      "source": [
        "def get_transform_from_keypoints(img_src, img_dst):\n",
        "  # Create a SIFT object and detect keypoints and descriptors for each image\n",
        "  sift=cv2.SIFT_create(nOctaveLayers=10)\n",
        "  kpts_src,dscrpt_src=sift.detectAndCompute(img_src,None)\n",
        "  kpts_dst,dscrpt_dst=sift.detectAndCompute(img_dst,None)\n",
        "\n",
        "  # Match the descriptors\n",
        "  matcher = cv2.BFMatcher()\n",
        "  # src기준으로 dst에서 가장 가까운 이웃과 그 다음으로 가까운 이웃을 반환한다(k=2).\n",
        "  matches = matcher.knnMatch(dscrpt_src, dscrpt_dst, k=2)  \n",
        "\n",
        "  # 두번째 distance와 ratio의 곱보다 첫번쨰 distance가 작으면 good match\n",
        "  # ratio가 작아질수록 첫번째 distance가 압도적으로 적어 매칭관련성이 매우 높다는것을 뜻한다.\n",
        "  good_matches = []\n",
        "  ratio=0.12\n",
        "  for m, n in matches:\n",
        "      if m.distance < ratio * n.distance:  \n",
        "          good_matches.append(m)\n",
        "  # print(good_matches[0].distance,good_matches[1].distance)\n",
        "  # good_matches에는 좋은 매칭들이 포함되어 있다\n",
        "  \n",
        "  # 좋은 매칭에 대해 queryIdx와 trainIdx를 사용하여 원본 이미지와 대상 이미지의 키포인트 좌표를 추출\n",
        "  src_pts = np.float32([kpts_src[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "  dst_pts = np.float32([kpts_dst[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "  \n",
        "  print(src_pts)\n",
        "  print(dst_pts)\n",
        "  # Find the homography matrix using the matched keypoints\n",
        "  H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)\n",
        "\n",
        "  # return outputs\n",
        "  return H, kpts_src, dscrpt_src, kpts_dst, dscrpt_dst, good_matches\n",
        "\n",
        "# image1=cv2.cvtColor(image1,cv2.COLOR_BGR2GRAY)\n",
        "# image2=cv2.cvtColor(image2,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "H, kpts_src, dscrpt_src, kpts_dst, dscrpt_dst, matches=get_transform_from_keypoints(image1,image2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_matching_results(img_src, img_dst, kpts_src, kpts_dst, matches):\n",
        "    # Draw the matches between the source and destination images\n",
        "    matched_image = cv2.drawMatches(img_src, kpts_src, img_dst, kpts_dst, matches, None)\n",
        "    #matched_image=cv2.resize(matched_image,(matched_image.shape[1]//2,matched_image.shape[0]//2))\n",
        "\n",
        "    # Display the image with the matches\n",
        "    cv2.imshow('Matching Results', matched_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "    \n",
        "draw_matching_results(image1,image2,kpts_src,kpts_dst,matches)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VLa2-PdCN4sj"
      },
      "source": [
        "Problem 2: [Code by yourself]\n",
        "\n",
        "Define a function named `stitch_image()` that generates a stitched image from img_ref, img_align, and T, with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (input): computed linear transform matrix\n",
        "* stitched_image (output): the stitched image\n",
        "\n",
        "This function should include the following processes:\n",
        "* Compute the size of the output stitched image and the offset that ensures all pixels from both images are included in the stitched image\n",
        "* Modify H to account for the image offset\n",
        "* Warp the first src image into the second image\n",
        "* Blend in the second dst image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-eV8gu5NwAC"
      },
      "outputs": [],
      "source": [
        "def get_stitched_image(img_src, img_dst, H):\n",
        "  # Compute the size of the output stitched image\n",
        "  ### CODE YOURSELF ###\n",
        "\n",
        "  \n",
        "  # Modify H to account for the image offset\n",
        "  ### CODE YOURSELF ###\n",
        "\n",
        "  # Warp the first image to the perspective of the second image\n",
        "  ### CODE YOURSELF ###\n",
        "\n",
        "  # Combine the two images to create a single stitched image\n",
        "  ### CODE YOURSELF ###\n",
        "\n",
        "  # return output\n",
        "  return stitched_image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "94CDChs3gh5V"
      },
      "source": [
        "Given code: \n",
        "3. Call function `get_transform_from_keypoints` - detect and match keypoints and compute transform\n",
        "4. Draw the matches on a new image to check validity of matched keypoints\n",
        "5. Call function `get_stitched_image` - create stitched image and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU2wERZJgiDq"
      },
      "outputs": [],
      "source": [
        "# 3. Detect and match keypoints and compute transform\n",
        "H, kpts_src, _, kpts_dst, _, matches = get_transform_from_keypoints(image1, image2)\n",
        "\n",
        "# 4. Draw the matches on a new image to check validity of matched keypoints\n",
        "match_image = cv2.drawMatches(image1, kpts_src, image2, kpts_dst, matches, None)\n",
        "cv2.imwrite('matches.png', match_image)\n",
        "\n",
        "# 3. Create stitched image and save\n",
        "stitched_image = get_stitched_image(image1, image2, H)\n",
        "cv2.imwrite('stitched.png', stitched_image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
